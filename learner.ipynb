{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435febec",
   "metadata": {},
   "source": [
    "# Learnability Project: PFA Phonotactic Learner\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "# General functions and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847077f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfoma import FST, State\n",
    "from math import log, exp\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "lam = \"Î»\"\n",
    "\n",
    "\n",
    "def data_iterator(path):\n",
    "    \"\"\"Yields a word as a list of tokens from a corpus\"\"\"\n",
    "    with open (path, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            yield line.split()\n",
    "              \n",
    "def make_alphabet(path) -> list:\n",
    "    \"\"\"This function extracts an alphabet from a corpusl\"\"\"\n",
    "    alph = []\n",
    "    for word in data_iterator(path):\n",
    "        for sym in word:\n",
    "            if sym not in alph:\n",
    "                alph.append(sym)\n",
    "    return alph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e70fc",
   "metadata": {},
   "source": [
    "# Strictly 2-Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50f441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_SL2_dfa(alphabet:list) -> list[FST]:\n",
    "    \"\"\"This function initializes a SL2 DFA according to an alphabet. All weights are set to 0 by default\"\"\"\n",
    "    \n",
    "    # create the FST object and instantiate the initial state\n",
    "    dfa = FST()\n",
    "    q0 = dfa.initialstate\n",
    "    q0.finalweight = 0\n",
    "    q0.name = lam\n",
    "    states = {q0,}\n",
    "    \n",
    "    # initialize all states and connect q0 to them\n",
    "    for sym in alphabet:\n",
    "        q = State()\n",
    "        q.name = sym\n",
    "        q0.add_transition(q, sym, 0)\n",
    "        states.add(q)\n",
    "        q.finalweight = 0\n",
    "    \n",
    "    # loopify the states, excluding q0\n",
    "    states_no_q0 = set([s for s in states if s.name != lam])\n",
    "    for state1 in states_no_q0:\n",
    "        for state2 in states_no_q0:\n",
    "            state1.add_transition(state2, state2.name, 0)\n",
    "    \n",
    "    dfa.states      = states\n",
    "    dfa.finalstates = states\n",
    "    dfa.alphabet    = alphabet\n",
    "    \n",
    "    return [dfa]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0b535",
   "metadata": {},
   "source": [
    "# Strictly 2-Piecewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5ff15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_SP2_dfas(alphabet:list) -> list[FST]:\n",
    "    \"\"\"This function intitializes SP2 machines according the length of the alphabet. Weights are 0 by default\"\"\"\n",
    "    SP2_machines = []\n",
    "    \n",
    "    # initializes all 2 state machines and loopify them\n",
    "    for sym1 in alphabet:\n",
    "        dfa             = FST()\n",
    "        q0              = dfa.initialstate\n",
    "        q0.finalweight  = 0\n",
    "        q0.name         = lam\n",
    "        q1              = State()\n",
    "        q1.finalweight  = 0\n",
    "        q1.name         = sym1\n",
    "        dfa.states      = {q0, q1}\n",
    "        dfa.finalstates = {q0, q1}\n",
    "        dfa.alphabet    = alphabet\n",
    "        q0.add_transition(q1, sym1, 0)\n",
    "        \n",
    "        # loopification\n",
    "        for sym2 in alphabet:\n",
    "            if sym1 != sym2:\n",
    "                q0.add_transition(q0, sym2, 0)\n",
    "            \n",
    "            q1.add_transition(q1, sym2, 0)\n",
    "        \n",
    "        SP2_machines.append(dfa)\n",
    "     \n",
    "    return SP2_machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b338f6",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e3aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transitions(dfas:list[FST], path:str) -> list[FST]:\n",
    "    \"\"\"This function updates one or more DFAs with relative frequencies from a given corpus\"\"\"\n",
    "    for word in data_iterator(path):\n",
    "        for dfa in dfas:\n",
    "            cs = dfa.initialstate # gets set back to q0 when a new word is processed\n",
    "            \n",
    "            for token in word:\n",
    "                for _, trans in cs.all_transitions():\n",
    "\n",
    "                    if trans.label == token: # find the transition that matches the current token\n",
    "                        trans.weight += 1\n",
    "                        \n",
    "                    if token == trans.targetstate.name: # update cs to transition's target state. This is how we \"traverse\" the states\n",
    "                        cs = trans.targetstate\n",
    "                                 \n",
    "            cs.finalweight += 1 # update the weight of the last state the current string ends in\n",
    "            \n",
    "    return dfas\n",
    "\n",
    "def get_trans_sum(state):\n",
    "    \"\"\"sums all the transition weights (counts) and the finalweight for given state \"\"\"\n",
    "    return sum([tran.weight for _, tran in state.all_transitions()])\n",
    "\n",
    "\n",
    "def assert_sum_to_1(dfa:FST) -> None:\n",
    "    \"\"\"This function asserts that each state's transition weights sum to 1 for given DFA\"\"\"\n",
    "    for state in dfa.states:\n",
    "        try:\n",
    "            total = get_trans_sum(state) + state.finalweight\n",
    "            assert total > .99 # accounts for floating point precision\n",
    "\n",
    "        except AssertionError:\n",
    "            print(f\"State: {state.name} is misbehaving!\\nWeighted sum: {total} != 1\")\n",
    "            raise RuntimeError\n",
    "\n",
    "def MLE(dfas:list[FST], path:str) -> list[FST]:\n",
    "    \"\"\"This function takes one or more dfas, updates the weights from a corpus, and normalizes\"\"\"\n",
    "    dfas = update_transitions(dfas, path)\n",
    "    \n",
    "    for dfa in dfas:\n",
    "        all_state_trans_sums = [get_trans_sum(state) + state.finalweight for state in dfa.states]# list of state weight sums for current dfa\n",
    "\n",
    "        for i, state in enumerate(dfa.states):\n",
    "            for _, trans in state.all_transitions():\n",
    "\n",
    "                #  normalize transition weight by sum of the state's transition weights\n",
    "                trans.weight = (trans.weight) / all_state_trans_sums[i]\n",
    "\n",
    "            state.finalweight = (state.finalweight / all_state_trans_sums[i])\n",
    "\n",
    "        assert_sum_to_1(dfa)\n",
    "    return dfas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b93056",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "For the datasets, the evaluation procedures are as follows:\n",
    "- For the toy dataset, in the `TestingData_toy.txt` file, there are ten (legal, illegal) word pairs. Performance is measured based on the average of the **legal-illegal difference** scores. A more positive value indicates that the legal string is receiving higher probability (Dai & Futrell, 2022).\n",
    "\n",
    "- For the Navajo and Quechua datasets, the `TestingData_navajo.txt` and `TestingData_quecha.txt` contain either legal or illegal words. To measure performance, the sum of the log likelihood of all legal words is compared to the sum of the log likelihood of all illegal words. The hypothesis is that the sum of legal words should be *greater than* the sum of illegal words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44f14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning data paths\n",
    "toy     = \"data/LearningData_toy.txt\"\n",
    "navajo  = \"data/LearningData_navajo.txt\"\n",
    "quechua = \"data/LearningData_quechua.txt\"\n",
    "\n",
    "toy_alph     = make_alphabet(toy)\n",
    "navajo_alph  = make_alphabet(navajo)\n",
    "quechua_alph = make_alphabet(quechua)\n",
    "\n",
    "def process_word(dfas:list[FST], word:list[str]):\n",
    "    \"\"\"Retrieves the log likelihood of a single word given one or more DFAs\"\"\"\n",
    "    logprobs = []\n",
    "    \n",
    "    for dfa in dfas:\n",
    "        cs = dfa.initialstate \n",
    "        for token in word:\n",
    "            for _, trans in cs.all_transitions():\n",
    "                \n",
    "                if trans.label == token:\n",
    "                    try:\n",
    "                        logprobs.append(log(trans.weight))\n",
    "                    except ValueError: # if sequence doesn't exist, trans.weight == 0, so logging this crashes\n",
    "                        logprobs.append(trans.weight)\n",
    "                        \n",
    "                if token == trans.targetstate.name: \n",
    "                    cs = trans.targetstate\n",
    "                \n",
    "    return sum(logprobs)\n",
    "\n",
    "def read_eval(path, delim):\n",
    "    \n",
    "    with open(path, \"r\") as fin:\n",
    "        reader = csv.reader(fin, delimiter=delim)\n",
    "        for row in reader:\n",
    "            yield row  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646c640",
   "metadata": {},
   "source": [
    "## Toy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbbe8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_SL2     = make_SL2_dfa(toy_alph)\n",
    "toy_SL2_MLE = MLE(toy_SL2, toy)\n",
    "\n",
    "toy_SP2     = make_SP2_dfas(toy_alph)\n",
    "toy_SP2_MLE = MLE(toy_SP2, toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47e8192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL2 Score: -0.6333024198097303\n",
      "SP2 Score: -0.5260450150019949\n"
     ]
    }
   ],
   "source": [
    "SL2_scores = []\n",
    "SP2_scores = []\n",
    "\n",
    "for legal, illegal in read_eval(\"data/TestingData_toy.txt\", delim=\",\"):\n",
    "    SL2_scores.append(process_word(toy_SL2_MLE, legal) - process_word(toy_SL2_MLE, illegal))\n",
    "    SP2_scores.append(process_word(toy_SP2_MLE, legal) - process_word(toy_SP2_MLE, illegal))\n",
    "    \n",
    "\n",
    "print(f\"SL2 Score: {np.mean(SL2_scores)}\")\n",
    "print(f\"SP2 Score: {np.mean(SP2_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de05253",
   "metadata": {},
   "source": [
    "## Navajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac008404",
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_SL2     = make_SL2_dfa(navajo_alph)\n",
    "nav_SL2_MLE = MLE(nav_SL2, navajo)\n",
    "\n",
    "nav_SP2     = make_SP2_dfas(navajo_alph)\n",
    "nav_SP2_MLE = MLE(nav_SP2, navajo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d7ec455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL2: \n",
      "Legal word score: -35156.98666530183\n",
      "Illegal word score: -66877.02525153007\n",
      "\n",
      "\n",
      "SP2: \n",
      "Legal word score: -2602164.941207403\n",
      "Illegal word score: -4872983.739561507\n"
     ]
    }
   ],
   "source": [
    "legal_SL2_scores = []\n",
    "illegal_SL2_scores = []\n",
    "\n",
    "legal_SP2_scores = []\n",
    "illegal_SP2_scores = []\n",
    "\n",
    "for word, label in read_eval(\"data/TestingData_navajo.txt\", delim=\"\\t\"):\n",
    "    \n",
    "    if label == \"legal\":\n",
    "        legal_SL2_scores.append(process_word(nav_SL2_MLE, word))\n",
    "        legal_SP2_scores.append(process_word(nav_SP2_MLE, word))\n",
    "    elif label == \"illegal\":\n",
    "        illegal_SL2_scores.append(process_word(nav_SL2_MLE, word))\n",
    "        illegal_SP2_scores.append(process_word(nav_SP2_MLE, word))\n",
    "        \n",
    "print(f\"SL2: \\nLegal word score: {sum(legal_SL2_scores)}\\nIllegal word score: {sum(illegal_SL2_scores)}\\n\\n\")\n",
    "print(f\"SP2: \\nLegal word score: {sum(legal_SP2_scores)}\\nIllegal word score: {sum(illegal_SP2_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc218612",
   "metadata": {},
   "source": [
    "## Quechua "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b31e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_SL2     = make_SL2_dfa(quechua_alph)\n",
    "que_SL2_MLE = MLE(que_SL2, quechua)\n",
    "\n",
    "que_SP2     = make_SP2_dfas(quechua_alph)\n",
    "que_SP2_MLE = MLE(que_SP2, quechua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71595d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL2: \n",
      "Legal word score: -155396.00004299358\n",
      "Illegal word score: -43021.74540275179\n",
      "\n",
      "\n",
      "SP2: \n",
      "Legal word score: -8449660.308918558\n",
      "Illegal word score: -2227730.152033898\n"
     ]
    }
   ],
   "source": [
    "legal_SL2_scores = []\n",
    "illegal_SL2_scores = []\n",
    "\n",
    "legal_SP2_scores = []\n",
    "illegal_SP2_scores = []\n",
    "\n",
    "for word, label in read_eval(\"data/TestingData_quechua.txt\", delim=\"\\t\"):\n",
    "    \n",
    "    if label.startswith(\"legal\"):\n",
    "        legal_SL2_scores.append(process_word(nav_SL2_MLE, word))\n",
    "        legal_SP2_scores.append(process_word(nav_SP2_MLE, word))\n",
    "    elif label.startswith(\"illegal\"):\n",
    "        illegal_SL2_scores.append(process_word(nav_SL2_MLE, word))\n",
    "        illegal_SP2_scores.append(process_word(nav_SP2_MLE, word))\n",
    "        \n",
    "print(f\"SL2: \\nLegal word score: {sum(legal_SL2_scores)}\\nIllegal word score: {sum(illegal_SL2_scores)}\\n\\n\")\n",
    "print(f\"SP2: \\nLegal word score: {sum(legal_SP2_scores)}\\nIllegal word score: {sum(illegal_SP2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff69833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
